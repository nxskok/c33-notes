
## Duality between confidence intervals and hypothesis tests 
- Tests and CIs really do the same thing, if you look at them the right
way. They are both telling you something about a parameter, and
they use same things about data.
- To illustrate, some data (two groups):
```{r}
my_url <- "http://www.utsc.utoronto.ca/~butler/c32/duality.txt"
twogroups <- read_delim(my_url," ")
```


## The data

\footnotesize
```{r}
twogroups
```
\normalsize

## 95% CI (default)
```{r}
t.test(y ~ group, data = twogroups)
```

## 90% CI
```{r}
t.test(y ~ group, data = twogroups, conf.level = 0.90)
```

## Hypothesis test

Null is that difference in means is zero:

```{r}
t.test(y ~ group, mu=0, data = twogroups)
```


## Comparing results

Recall null here is $H_0 : \mu_1 - \mu_2 = 0$. P-value 0.0668. 

- 95% CI from $-5.6$ to 0.2, contains 0.
- 90% CI from $-5.0$ to $-0.3$, does not contain 0.
- At $\alpha = 0.05$, would not reject $H_0$ since P-value > 0.05.
- At $\alpha = 0.10$, would reject $H_0$ since P-value < 0.10.

Not just coincidence. Let $C = 100(1 - \alpha)$, so C% gives corresponding CI
to level-$\alpha$ test. Then following always true.
($\iff$ means ``if and only if''.)

\begin{tabular}{|rcl|}
  \hline
  Reject $H_0$ at level $\alpha$ & $\iff$ & $C\%$ CI does not contain $H_0$ value\\
  Do not reject $H_0$ at level $\alpha$ & $\iff$ & $C\%$ CI contains $H_0$ value\\
  \hline
\end{tabular}

Idea: "Plausible" parameter value inside CI, not rejected;
  "Implausible" parameter value outside CI, rejected. 
  
## The value of this
- If you have a test procedure but no corresponding CI:
- you make a CI by including all the parameter values that would not
be rejected by your test.
- Use:
  - $\alpha = 0.01$ for a 99% CI,
  - $\alpha = 0.05$ for a 95% CI,
  - $\alpha = 0.10$ for a 90% CI,
and so on.

## Testing for non-normal data
- The IRS (“Internal Revenue Service”) is the US authority that deals
with taxes (like Revenue Canada).
- One of their forms is supposed to take no more than 160 minutes to
complete. A citizen’s organization claims that it takes people longer
than that on average.
- Sample of 30 people; time to complete form recorded.
- Read in data, and do $t$-test of $H_0 : \mu = 160$ vs. $H_a : \mu > 160$.
- For reading in, there is only one column, so can pretend it is delimited
by anything.
  
## Read in data 
```{r}
my_url <- "http://www.utsc.utoronto.ca/~butler/c32/irs.txt"
irs <- read_csv(my_url)
irs %>% glimpse()
```

## Test whether mean is 160 or greater
```{r}
t.test(irs$Time, mu = 160, alternative = "greater")
```

Reject null; mean (for all people to complete form) greater than 160.

## But, look at a graph
```{r, fig.height=3.5}
ggplot(irs, aes(x = Time)) + geom_histogram(bins = 10)
```

## Comments

- Skewed to right. 
- Should look at *median*, not mean.

## The sign test
- But how to test whether the median is greater than 160?
- Idea: if the median really is 160 ($H_0$ true), the sampled values from
the population are equally likely to be above or below 160.
- If the population median is greater than 160, there will be a lot of
sample values greater than 160, not so many less. Idea: test statistic
is number of sample values greater than hypothesized median.
- How to decide whether “unusually many” sample values are greater
than 160? Need a sampling distribution.
- If $H_0$ true, pop. median is 160, then each sample value independently
equally likely to be above or below 160.
- So number of observed values above 160 has binomial distribution
with $n = 30$ (number of data values) and $p = 0.5$ (160 is
hypothesized to be *median*).

## Obtaining P-value for sign test 1/2

- Count values above/below 160:
```{r}
irs %>% count(Time > 160)
```

- 17 above, 13 below. How unusual is that? Need a *binomial table*.

## Obtaining P-value for sign test 2/2
- R function `dbinom` gives the probability of eg. exactly 17 successes in
a binomial with $n = 30$ and $p = 0.5$:
```{r}
dbinom(17, 30, 0.5)
```

- but we want probability of 17 *or more*, so get all of those, find probability of each,  and add them up: 
```{r}
tibble(x=17:30) %>% 
  mutate(prob=dbinom(x, 30, 0.5)) %>% 
  summarize(total=sum(prob))
```


## Using my package `smmr`
- I wrote a package `smmr` to do the sign test (and some other things).
Installation is a bit fiddly:
  - Install devtools with `install.packages("devtools")`
  - then install smmr: 
```{r, eval=F}
library(devtools)
install_github("nxskok/smmr")
```
- Then load it:
```{r, eval=F}
library(smmr)
```

## `smmr` for sign test
- `smmr`’s function `sign_test` needs three inputs: a data frame, a
column and a null median:
```{r}
sign_test(irs, Time, 160)
```

## Comments (1/3)

- Testing whether population median *greater than* 160, so want
*upper-tail* P-value 0.2923. Same as before.
- Also get table of values above and below; this too as we got.

## Comments (2/3)
- P-values are:
    \begin{center}
    \begin{tabular}{lr}
      Test & P-value\\
      \hline
      $t$ & 0.0392\\
      Sign & 0.2923\\
      \hline
    \end{tabular}
      
    \end{center}
- These are very different: we reject a mean of 160 (in favour of the
mean being bigger), but clearly *fail* to reject a median of 160 in
favour of a bigger one.
- Why is that? Obtain mean and median: 
```{r}
irs %>% summarize(mean = mean(Time), median = median(Time))
```

## Comments (3/3)
- The mean is pulled a long way up by the right skew, and is a fair bit
bigger than 160.
- The median is quite close to 160.
- We ought to be trusting the sign test and not the t-test here (median
and not mean), and therefore there is no evidence that the “typical”
time to complete the form is longer than 160 minutes.
- Having said that, there are clearly some people who take a lot longer
than 160 minutes to complete the form, and the IRS could focus on
simplifying its form for these people.
- In this example, looking at any kind of average is not really helpful; a
better question might be “do an unacceptably large fraction of people
take longer than (say) 300 minutes to complete the form?”: that is,
thinking about worst-case rather than average-case.

## Confidence interval for the median
- The sign test does not naturally come with a confidence interval for
the median.
- So we use the “duality” between test and confidence interval to say:
the (95%) confidence interval for the median contains exactly those
values of the null median that would not be rejected by the two-sided
sign test (at $\alpha = 0.05$).

## For our data
- The procedure is to try some values for the null median and see which
ones are inside and which outside our CI.
- smmr has pval_sign that gets just the 2-sided P-value:
```{r}
pval_sign(160, irs, Time)
```

- Try a couple of null medians:
```{r}
pval_sign(200, irs, Time)
pval_sign(300, irs, Time)
```

- So 200 inside the 95% CI and 300 outside.

## Doing a whole bunch
- Choose our null medians first:

\small
```{r}
(d=tibble(null_median=seq(100,300,20)))
```
\normalsize

## ... and then

“for
each null median, run the function `pval_sign` for that null median
and get the P-value”: 

```{r}
d %>% mutate(p_value = map_dbl(null_median, 
                               ~ pval_sign(., irs, Time)))
```

## Make it easier for ourselves 

```{r}
d %>% 
  mutate(p_value = map_dbl(null_median, 
                           ~ pval_sign(., irs, Time))) %>%
  mutate(in_out = ifelse(p_value > 0.05, "inside", "outside"))
```


## confidence interval for median?

- 95% CI to this accuracy from 120 to 200.
- Can get it more accurately by looking more closely in intervals from
100 to 120, and from 200 to 220.

## A more efficient way: bisection
- Know that top end of CI between 200 and 220:
```{r}
lo=200 
hi=220
```

- Try the value halfway between: is it inside or outside?
```{r}
(try = (lo + hi) / 2)
pval_sign(try,irs,Time)
```

- Inside, so upper end is between 210 and 220. Repeat (over):

## ... bisection continued 

```{r}
lo = try
(try = (lo + hi) / 2)
pval_sign(try, irs, Time)
```

- 215 is inside too, so upper end between 215 and 220. 
- Continue until have as accurate a result as you want.

## Bisection automatically

- A loop, but not a `for` since we don’t know how many times we’re
going around. Keep going while a condition is true:
```{r, eval=F}
lo = 200
hi = 220
while (hi - lo > 1) {
  try = (hi + lo) / 2
  ptry = pval_sign(try, irs, Time)
  print(c(try, ptry))
  if (ptry <= 0.05)
    hi = try
  else
    lo = try
}
```

## The output from this loop

```{r, echo=F}
lo = 200
hi = 220
while (hi - lo > 1) {
  try = (hi + lo) / 2
  ptry = pval_sign(try, irs, Time)
  print(c(try, ptry))
  if (ptry <= 0.05)
    hi = try
  else
    lo = try
}
```

- 215 inside, 215.625 outside. Upper end of interval to this accuracy is 215.

## Using smmr
- `smmr` has function `ci_median` that does this (by default 95% CI):
```{r}
ci_median(irs,Time)
```

- Uses a more accurate bisection than we did.
- Or get, say, 90% CI for median:
```{r}
ci_median(irs,Time,conf.level=0.90)
```

- 90% CI is shorter, as it should be.

## Matched pairs

Some data: 

\centering{
  \includegraphics[height=0.7\textheight]{Screenshot_2019-04-26_13-41-29}
}


## Matched pairs data
- Data are comparison of 2 drugs for effectiveness at reducing pain.
- 12 subjects (cases) were arthritis sufferers
- Response is #hours of pain relief from each drug.
- In reading example, each child tried only one reading method.
- But here, each subject tried out both drugs, giving us two
measurements.
- Possible because, if you wait long enough, one drug has no influence
over effect of other.
- Advantage: focused comparison of drugs. Compare one drug with
another on same person, removes a lot of variability due to differences between people. 
- Matched pairs, requires different analysis. 
- Design: randomly choose 6 of 12 subjects to get drug A first, other 6
get drug B first.

## Paired t test: reading the data
Values aligned in columns:  

```{r}
my_url <- "http://www.utsc.utoronto.ca/~butler/c32/analgesic.txt"
pain <- read_table(my_url)
```

## The data

```{r}
pain
```

## Paired *t*-test 

```{r}
with(pain, t.test(druga, drugb, paired = T))
```

P-value is 0.053. Likewise, you can calculate the differences yourself and
do a 1-sample t-test on them, over:

## t-testing the differences
- First calculate a column of differences (in data frame):
```{r}
(pain %>% mutate(diff=druga-drugb) -> pain)
```

## t-test on the differences
- then throw them into t.test, testing that the mean is zero, with
same result as before:
```{r}
with(pain,t.test(diff,mu=0))
```

## Assessing normality 
- 1-sample and 2-sample t-tests assume (each) group normally
distributed.
- Matched pairs analyses assume (theoretically) that differences
normally distributed.
- Though we know that t-tests generally behave well even without
normality.
- How to assess normality? A normal quantile plot.
  - Idea: scatter of points should follow the straight line, without curving.
  - Outliers show up at bottom left or top right of plot as points off the
line.

## The normal quantile plot

- of differences from matched pairs data

```{r, fig.height=3.3}
ggplot(pain,aes(sample=diff))+stat_qq()+stat_qq_line()
```

- Points should follow the straight line. Bottom left one way off, so
normality questionable here: outlier.

## More normal quantile plots
- How straight does a normal quantile plot have to be?
- There is randomness in real data, so even a normal quantile plot from
normal data won’t look perfectly straight.
- With a small sample, can look not very straight even from normal
data.
- Looking for systematic departure from a straight line; random wiggles
ought not to concern us.
- Look at some examples where we know the answer, so that we can
see what to expect.

## Normal data, large sample

```{r set-seed, echo=F}
set.seed(457299)
```


```{r, fig.height=3.5}
d=tibble(x=rnorm(200))
ggplot(d,aes(x=x))+geom_histogram(bins=10)
```

## The normal quantile plot

```{r, fig.height=3.8}
ggplot(d,aes(sample=x))+stat_qq()+stat_qq_line()
```

## Normal data, small sample

```{r, echo=F}
set.seed(457299)
```

- Not so convincingly normal, but not obviously skewed:


```{r normal-small, fig.height=3.1}
d=tibble(x=rnorm(20))
ggplot(d,aes(x=x))+geom_histogram(bins=5)
```


## The normal quantile plot

Good, apart from the highest and lowest points being slightly off. I’d call
this good:


```{r, fig.height=3.1}
ggplot(d,aes(sample=x))+stat_qq()+stat_qq_line()
```

## Chi-squared data, *df* = 10

Somewhat skewed to right:

```{r, fig.height=3.1}
d=tibble(x=rchisq(100,10))
ggplot(d,aes(x=x))+geom_histogram(bins=10)
```



## The normal quantile plot

Somewhat opening-up curve:

```{r, fig.height=3.3}
ggplot(d,aes(sample=x))+stat_qq()+stat_qq_line()
```


## Chi-squared data, df = 3

Definitely skewed to right:

```{r chisq-small-df, fig.height=3.1}
d=tibble(x=rchisq(100,3))
ggplot(d,aes(x=x))+geom_histogram(bins=10)
```



## The normal quantile plot

Clear upward-opening curve:

```{r, fig.height=3.4}
ggplot(d,aes(sample=x))+stat_qq()+stat_qq_line()
```



## t-distributed data, df = 3

Long tails (or a very sharp peak):

```{r t-small, fig.height=3.2}
d=tibble(x=rt(300,3))
ggplot(d,aes(x=x))+geom_histogram(bins=10)
```



## The normal quantile plot

Low values too low and high values too high for normal.

```{r, fig.height=3.4}
ggplot(d,aes(sample=x))+stat_qq()+stat_qq_line()
```


## Our pain-relief data

```{r pain-relief-qq, fig.height=3.8}
ggplot(pain,aes(sample=diff))+stat_qq()+stat_qq_line()
```

## Comments

- Definitely not normal. What to do?
- Sign test on differences, null median 0.

## Sign test
- Most easily: calculate differences in data frame, then use `smmr`.
- Null median difference is 0:

```{r}
pain %>% mutate(mydiff=druga-drugb) %>%
sign_test(mydiff,0)
```



## Comments 

- P-value 0.1460. No evidence that the drugs are different.
- Since we are working in a pipeline, input data frame to `sign_test` is
“whatever came out of previous step”.


## (Some of) the kids’ reading data, again

```{r}
kids %>% sample_n(12)
```

## Where we are at 

- 21 kids in “treatment”, new reading method; 23 in “control”,
standard reading method.

- Assessing assumptions:
  - We did two-sample t-test (Satterthwaite-Welch) before.
  - Assumes approx. normal data within each group.
  - Does not assume equal spread.
  - (Pooled t-test *does* assume equal spread).
  - Assess each group separately. 

## Boxplots for reading data

```{r, fig.height=3.7}
ggplot(kids,aes(x=group,y=score))+geom_boxplot()
```

## Facetted normal quantile plots
Done this way:

```{r, fig.height=3.2}
ggplot(kids,aes(sample=score))+stat_qq()+stat_qq_line()+
facet_wrap(~group)
```

## Comments
- These plots show no problems with normality. Both groups are more
or less symmetric/normal and there are no outliers.
- Equal spreads questionable, but we don’t need that.
- Assess equal spreads by looking at *slopes* of normal quantile plots.
- We ought be happy with the (Welch) two-sample t-test (over)


## Welch two-sample test

```{r}
t.test(score~group,data=kids,alternative="less")
```

from which we concluded that the new reading method really does
help.

## What to do if normality fails
- (On the previous page, the only indication of non-normality is the
highest score in the control group, which is a little too high for
normality.)
- If normality fails (for one or both of the groups), what do we do then?
- Again, can compare medians: use the thought process of the sign test,
which does not depend on normality and is not damaged by outliers.
- A suitable test called Mood’s median test.
- Before we get to that, a diversion.

## The chi-squared test for independence

Suppose we want to know whether people are in favour of having
daylight savings time all year round. We ask 20 males and 20 females
whether they each agree with having DST all year round (“yes”) or
not (“no”). Some of the data: 

```{r, message=F}
my_url <- "http://www.utsc.utoronto.ca/~butler/c32/dst.txt"
dst <- read_delim(my_url," ")
dst %>% sample_n(5) # randomly sample 5 rows
```

## ... continued

Count up individuals in each category combination, and arrange in
contingency table:
```{r}
tab=with(dst,table(gender,agree))
tab
```

- Most of the males say “yes”, but the females are about evenly split.
- Looks like males more likely to say “yes”, ie. an association between
gender and agreement.
- Test an $H_0$ of “no association” (“independence”) vs. alternative that
there is really some association. 
- Done with `chisq.test`.

## ...And finally

```{r}
chisq.test(tab,correct=F)
```

- Reject null hypothesis of no association
- therefore there is a difference in rates of agreement between (all)
males and females (or that gender and agreement are associated).
- Without `correct=F` uses “Yates correction”; this way, should give
same answers as calculated by hand (if you know how).

## Mood’s median test
- Before our diversion, we wanted to compare medians of two groups.
- Recall sign test: count number of values above and below something
(there, hypothesized median).
- Idea of Mood’s median test:
  - Work out the median of all the data, regardless of group (“grand
median”).
  - Count how many data values in each group are above/below this grand
median.
  - Make contingency table of group vs. above/below.
  - Test for association.
- If group medians equal, each group should have about half its
observations above/below grand median. If not, one group will be
mostly above grand median and other below.

## Mood’s median test for reading data
- Find overall median score: 
```{r}
(kids %>% summarize(med=median(score)) %>% pull(med) -> m)
```

- Make table of above/below vs. group:
```{r}
tab=with(kids,table(group,score>m))
tab
```


- Treatment group scores mostly above median, control group scores
mostly below, as expected.

## The test
- Do chi-squared test:
```{r}
chisq.test(tab,correct=F)
```


- This test actually two-sided (tests for any association). 
- Here want to test that new reading method *better* (one-sided).
- Most of treatment children above overall median, so
do 1-sided test by halving P-value to get 0.017. 
- This way too, children do better at learning to read using the new
method.

## Or by smmr
- `median_test` does the whole thing:

```{r}
median_test(kids,score,group)
```

- P-value again two-sided.

## Comments
- P-value 0.013 for (1-sided) t-test, 0.017 for (1-sided) Mood median
test.
- Like the sign test, Mood’s median test doesn’t use the data very
efficiently (only, is each value above or below grand median).
- Thus, if we can justify doing *t*-test, we should do it. This is the case
here.
- The *t*-test will usually give smaller P-value because it uses the data
more efficiently.
- The time to use Mood’s median test is if we are definitely unhappy
with the normality assumption (and thus the t-test P-value is not to
be trusted).

## Jumping rats
- Link between exercise and healthy bones (many studies).
- Exercise stresses bones and causes them to get stronger.
- Study (Purdue): effect of jumping on bone density of growing rats.
- 30 rats, randomly assigned to 1 of 3 treatments:
  - No jumping (control)
  - Low-jump treatment (30 cm)
  - High-jump treatment (60 cm)
- 8 weeks, 10 jumps/day, 5 days/week.
- Bone density of rats (mg/cm 3 ) measured at end.
- See whether larger amount of exercise (jumping) went with higher
bone density.
- Random assignment: rats in each group similar in all important ways.
- So entitled to draw conclusions about cause and effect.

## Reading the data
Values separated by spaces:
```{r}
my_url <- "http://www.utsc.utoronto.ca/~butler/c32/jumping.txt"
rats <- read_delim(my_url," ")
```

## The data (some random rows) 

\small
```{r}
rats %>% sample_n(12)
```
\normalsize

## Boxplots

```{r, fig.height=3.7}
ggplot(rats,aes(y=density,x=group))+geom_boxplot()
```

## Or, arranging groups in data (logical) order

```{r, fig.height=3.5}
ggplot(rats,aes(y=density,x=fct_inorder(group)))+
geom_boxplot()
```

## Analysis of Variance
- Comparing > 2 groups of independent observations (each rat only
does one amount of jumping).
- Standard procedure: analysis of variance (ANOVA).
- Null hypothesis: all groups have same mean.
- Alternative: “not all means the same”, at least one is different from
others.

## Testing: ANOVA in R

```{r}
rats.aov=aov(density~group,data=rats)
summary(rats.aov)
```

- Usual ANOVA table, small P-value: significant result.
- Conclude that the mean bone densities are not all equal.
- Reject null, but not very useful finding.

## Which groups are different from which?
- ANOVA really only answers half our questions: it says “there are
differences”, but doesn’t tell us which groups different.
- One possibility (not the best): compare all possible pairs of groups,
via two-sample t.
- First pick out each group:

```{r}
rats %>% filter(group=="Control") -> controls
rats %>% filter(group=="Lowjump") -> lows
rats %>% filter(group=="Highjump") -> highs
```

## Control vs. low

```{r}
t.test(controls$density,lows$density)
```

No sig. difference here.

## Control vs. high

```{r}
t.test(controls$density,highs$density)
```

These are different.

## Low vs. high

```{r}
t.test(lows$density,highs$density)
```

These are different too.

## But...
- We just did 3 tests instead of 1.
- So we have given ourselves 3 chances to reject $H_0:$ all means equal,
instead of 1.
- Thus $\alpha$ for this combined test is not 0.05.

## John W. Tukey 

  \begin{columns}
    \begin{column}{0.4\textwidth}
      \includegraphics[width=\textwidth]{John_Tukey}
    \end{column}
    \begin{column}{0.6\textwidth}
      \begin{itemize}
      \item American statistician, 1915--2000
      \item Big fan of exploratory data analysis
      \item Invented boxplot
      \item Invented "honestly significant differences"
      \item Invented jackknife estimation
      \item Coined computing term "bit"
      \item Co-inventor of Fast Fourier Transform
      \end{itemize}
    \end{column}
  \end{columns}
  

## Honestly Significant Differences
- Compare several groups with one test, telling you which groups differ
from which.
- Idea: if all population means equal, find distribution of highest sample
mean minus lowest sample mean.
- Any means unusually different compared to that declared significantly
different.

## Tukey on rat data

```{r, echo=F}
width=getOption("width")
options(width=60)
```


\small
```{r}
rats.aov=aov(density~group,data=rats)
TukeyHSD(rats.aov)
```
\normalsize

```{r, echo=F}
options(width=width)
```

- Again conclude that bone density for highjump group significantly higher
than for other two groups.

## Why Tukey’s procedure better than all t-tests 
Look at P-values for the two tests:

```
Comparison        Tukey    t-tests
----------------------------------
Highjump-Control 0.0016     0.0021
Lowjump-Control  0.4744     0.2977
Lowjump-Highjump 0.0298     0.0045
```

  
- Tukey P-values (mostly) higher.
- Proper adjustment for doing three t-tests at once, not just one in
isolation.
- `lowjump-highjump` comparison would no longer be significant at
$\alpha = 0.01$.

## Checking assumptions

```{r, fig.height=3.5}
ggplot(rats,aes(y=density,x=fct_inorder(group)))+
geom_boxplot()
```

Assumptions:
- Normally distributed data within each group
- with equal group SDs.

## Normal quantile plots by group

```{r, fig.height=3.5}
ggplot(rats, aes(sample = density)) + stat_qq() + 
  stat_qq_line() + facet_wrap( ~ group)
```

## The assumptions
- Normally-distributed data within each group
- Equal group SDs.
These are shaky here because:
- control group has outliers
- highjump group appears to have less spread than others.
Possible remedies (in general):
- Transformation of response (usually works best when SD increases
with mean)
- If normality OK but equal spreads not, can use Welch ANOVA.
(Regular ANOVA like pooled t-test; Welch ANOVA like
Welch-Satterthwaite t-test.)
- Can also use Mood’s Median Test (see over). This works for any
number of groups.

## Mood’s median test 1/4
- Find median of all bone densities, regardless of group:

\small
```{r}
(rats %>% summarize(med = median(density)) %>% pull(med) -> m)
```
\normalsize

- Count up how many observations in each group above or below
overall median:

```{r}
tab = with(rats, table(group, density > m))
tab
```


## Mood’s median test 2/4 

```{r}
tab
```


- All Highjump obs above overall median.
- Most Control obs below overall
median.
- Suggests medians differ by group.

## Mood’s median test 3/4 
- Test whether association between group and being above/below
overall median significant using chi-squared test for association:

```{r}
chisq.test(tab,correct=F)
```

- Very small P-value says that being above/below overall median
depends on group.
- That is, groups do not all have same median.

## Mood’s median test 4/4 
Or with `median_test` from `smmr`, same as before. 

```{r}
median_test(rats,density,group)
```



## Comments
- No doubt that medians differ between groups (not all same). 
- This test is equivalent of $F$-test, not of Tukey. 
- To determine which groups differ from which, can compare all possible
pairs of groups via (2-sample) Mood’s median tests, then adjust
P-values by multiplying by number of 2-sample Mood tests done (Bonferroni):

```{r}
pairwise_median_test(rats,density,group)
```

- Now, lowjump-highjump difference no longer significant. 

## Welch ANOVA
- For these data, Mood’s median test probably best because we doubt
both normality and equal spreads.
- When normality OK but spreads differ, Welch ANOVA way to go.
- Welch ANOVA done by `oneway.test` as shown (for illustration):

```{r}
oneway.test(density~group,data=rats)
```

- P-value very similar, as expected.
- Appropriate Tukey-equivalent here called Games-Howell.

## Games-Howell

- Lives in package `PMCMRplus` (also `userfriendlyscience`). Install
first.

```{r, eval=F}
library(PMCMRplus)
```


```{r games-howell, warning=F}
gamesHowellTest(density~factor(group),data=rats)
```



